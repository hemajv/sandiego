{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to identify important repositories using graphical analysis on historical data\n",
    "\n",
    "In the [graph analysis notebook](./graph_analysis.ipynb), we explored relationships between open source projects and communities by studying graphs. We explored relations such as common contributors and project activities between different GitHub repositories.\n",
    "\n",
    "In this notebook, our aim is to leverage the graph representation techniques we explored in the previous notebook and apply algorithms such as PageRank, Betweenness Centrality to find important emerging projects.\n",
    "\n",
    "In order to do that, we test these algorithms on historical data of well known open source project communities such as OpenShift and Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Augur database\n",
    "\n",
    "We will be fetching the data from an Augur database which stores the GitHub data for a large number of open source repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "\n",
    "import sqlalchemy as salc\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from ipynb.fs.defs.graph_helper_functions import (\n",
    "     get_repos,\n",
    "     get_issue_contributors,\n",
    "     get_pr_contributors,\n",
    "     get_commit_contributors,\n",
    "     get_prr_contributors,\n",
    "     created_melted_dfs\n",
    ")\n",
    "\n",
    "with open(\"../../comm_cage.json\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Available Repositories for OpenShift and Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with known project communities during the years [2011-2014](https://developer.ibm.com/blogs/a-brief-history-of-red-hat-openshift/)  and color the project nodes differently as we plot the graphs. We categorize repositories into 3 buckets. \n",
    "\n",
    "1. **Well-known**: We categorize Kubernetes and Docker repos in this bucket. We assume that we are aware of the Kubernetes and Docker communities and we wish to reveal related emerging communities which are related to Kubernetes\n",
    "\n",
    "2. **Emerging**: We categorize OpenShift repos in this bucket. We go forward with the assumption that there is an overlap of contributors between the OpenShift and Kubernetes repositories. We can then apply the graph algorithms for the Kubernetes repos to help discover the OpenShift community.\n",
    "\n",
    "3. **Other Communities**: In this bucket, we will include other repositories belonging to un-related project communities which either already are important communities or are also emerging in that time frame (2011-2014). We found some popular communities which emerged in the 2011-2014 time range as well as already popular repositories such as Apache Hadoop, Apache Mesos, Node, Eclipse jetty.project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./red_hat_repos.txt', 'r') as f:\n",
    "    rh_affiliated_projects = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing repositories based on a category\n",
    "\n",
    "interesting_projects = ['https://github.com/apache/kafka',\n",
    "                    'https://github.com/torvalds/linux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29380, 36109] ['kafka', 'linux']\n"
     ]
    }
   ],
   "source": [
    "repo_set_interesting, repo_name_set_interesting = get_repos(interesting_projects, engine)\n",
    "print(repo_set_interesting, repo_name_set_interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_set_rh_affiliated, repo_name_set_rh_affiliated = get_repos(rh_affiliated_projects, engine)\n",
    "print(repo_set_rh_affiliated, repo_name_set_rh_affiliated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_set = repo_set_rh_affiliated + repo_set_interesting \n",
    "repo_name_set = repo_name_set_rh_affiliated + repo_name_set_interesting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repo_set, repo_name_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Issue Contributors\n",
    "\n",
    "We will now fetch all Issue contributors for various repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_contrib = get_issue_contributors(repo_set, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_contrib['created_at_dt'] = issue_contrib['created_at'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = (issue_contrib['created_at_dt'] > pd.to_datetime('2011-01-01')) & (issue_contrib['created_at_dt'] <= pd.to_datetime('2014-06-30'))\n",
    "issue_contrib = issue_contrib.loc[date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve PR Contributors\n",
    "\n",
    "We will now fetch all the PR contributors for various repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_contrib = get_pr_contributors(repo_set, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_contrib['cntrb_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_contrib['cntrb_id'] = pr_contrib['cntrb_id'].astype('float').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_contrib['created_at_dt'] = pr_contrib['pr_created_at'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = (pr_contrib['created_at_dt'] > pd.to_datetime('2011-01-01')) & (pr_contrib['created_at_dt'] <= pd.to_datetime('2014-06-30'))\n",
    "pr_contrib = pr_contrib.loc[date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Commit Contributors\n",
    "\n",
    "We will now fetch all the Commit contributors for various repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_contrib = get_commit_contributors(repo_set, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_contrib['cmt_date_attempted_dt'] = commit_contrib['cmt_date_attempted'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = (commit_contrib['cmt_date_attempted_dt'] > pd.to_datetime('2011-01-01')) & (commit_contrib['cmt_date_attempted_dt'] <= pd.to_datetime('2014-06-30'))\n",
    "commit_contrib = commit_contrib.loc[date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve PR Reviewers \n",
    "\n",
    "We will now fetch all the PR Reviewers for various repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr_contrib = get_prr_contributors(repo_set, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Type 1: Projects and Contributors as Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we plot projects and contributors on the same graph as nodes and color them differently to see the relationships between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commit = commit_contrib.groupby(['repo_name', 'cntrb_id']).size().unstack(fill_value=0)\n",
    "df_commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataframe, each row represents a repository ID and each column represents a contributor. The dataframe contains counts for the number of times a contributor has made contributions to a particular repository. In the dataframe below `df_commit`, each contribution represents a commit. A value 0 means that a particular contributor has made no commits to the repository, and a a number x means that the contributor has made x number of commits to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commit = df_commit.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted_commit = df_commit.melt(\n",
    "    ['repo_name'],\n",
    "    var_name = 'cntrb_id',value_name='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted_commit = df_melted_commit[df_melted_commit[df_melted_commit.columns[2]] != 0]\n",
    "df_melted_commit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `df_melted_commit` we transpose the contributor IDs. Each row is a combination of a unique repository and a unique contributor and the number represents the number of times the contributor has made contributors to the particular repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create melted dataframes for all contribution type dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted_pr_contrib = created_melted_dfs(pr_contrib)\n",
    "df_melted_pr_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted_issue_contrib = created_melted_dfs(issue_contrib)\n",
    "df_melted_issue_contrib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted_prr_contrib = created_melted_dfs(prr_contrib)\n",
    "df_melted_prr_contrib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join repositories of all contribution types\n",
    "\n",
    "Combine dataframes of Issue contributors, PR contributors, PR Reviewers, and Commit Contributors to get aggregated contributor dataframes for each repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_issue = pd.concat([df_melted_commit, df_melted_issue_contrib]).groupby([\"repo_name\", \"cntrb_id\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_issue_pr_df = pd.concat([df_melted_pr_contrib, commit_issue]).groupby([\"repo_name\", \"cntrb_id\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([comm_issue_pr_df, df_melted_prr_contrib]).groupby([\"repo_name\", \"cntrb_id\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns = {'number':'total_contributions'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(merged_df, \n",
    "                            source='repo_name',\n",
    "                            target='cntrb_id',\n",
    "                            edge_attr='total_contributions',\n",
    "                            create_using=nx.Graph())\n",
    "\n",
    "Repo_name = merged_df['repo_name'].to_list()\n",
    "contributor_id = merged_df['cntrb_id'].to_list()\n",
    "\n",
    "nodes = G.nodes()\n",
    "\n",
    "colors = []\n",
    "for n in nodes:\n",
    "    if n in repo_name_set_rh_affiliated:\n",
    "        colors.append('blue')\n",
    "    elif n in repo_name_set_interesting:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('yellow')\n",
    "\n",
    "node_sizes = [800 if ns in repo_name_set else 10 for ns in nodes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='Contributor')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Red Hat Affiliated Repositories')\n",
    "green_patch = mpatches.Patch(color='green', label='Interesting Repositories')\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Graph representation of open-source repositories and their contributors\", font)\n",
    "\n",
    "ax.legend(handles=[yellow_patch, blue_patch, green_patch])\n",
    "nx.draw_networkx(G, with_labels=False, node_color=colors, font_size=10, node_size=node_sizes, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "Here we see that some of the Kubernetes, Docker & Openshift repositories seem to form a cluster. Some of the other repositories like the jettyproject also seem to have a lot of contributors, but they don't have any links/edges with the Kubernetes repositories. Lets analyze this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PageRank and Betweenness Centrality to Subset Nodes\n",
    "\n",
    "We can now try to run the `PageRank` algorithm to compute the ranking of the nodes in the graph based on the structure of the incoming links. \n",
    "\n",
    "We will also look into the betweenness centrality in the graph to compute the shortest-path betweenness centrality for nodes. It measures how often a node occurs on all shortest paths between two nodes. Here we are trying to analyze which are the common repositories that occur on all paths in the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank ranks important nodes by analyzing the quantity and quality of the links that point to it. In our case, links that point to repositories come from contributors. A unidirected graph from contributors to repositories will not be able to assign importances to well connected contributors. Hence we will create a bidirected graph where for each connection between a contributor and a repository, there is a 2 way arrow.\n",
    "\n",
    "Thus, we hope that PageRank ranks important repositories by analyzing the number of contributors connected to it as well as consider how well connected those contributors are within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirect_df = merged_df.append(merged_df.rename(columns={\"repo_name\":\"cntrb_id\",\"cntrb_id\":\"repo_name\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directed graph to run page rank\n",
    "H = nx.from_pandas_edgelist(bidirect_df, \n",
    "                            source='cntrb_id',\n",
    "                            target='repo_name',\n",
    "                            edge_attr='total_contributions',\n",
    "                            create_using=nx.DiGraph())\n",
    "\n",
    "nodes = H.nodes()\n",
    "\n",
    "Repo_name = merged_df['repo_name'].to_list()\n",
    "contributor_id = merged_df['cntrb_id'].to_list()\n",
    "node_sizes = [800 if ns in repo_name_set else 20 for ns in nodes]\n",
    "\n",
    "colors = []\n",
    "for n in nodes:\n",
    "    if n in repo_name_set_rh_affiliated:\n",
    "        colors.append('blue')\n",
    "    elif n in repo_name_set_interesting:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('yellow')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Bidirectional graph representation of open-source repositories and their contributors\", font)\n",
    "\n",
    "ax.legend(handles=[yellow_patch, blue_patch, green_patch])\n",
    "nx.draw_networkx(H, with_labels=False, node_color=colors, node_size=node_sizes, font_size=9, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "This plot is the same graph as the last one, only this one is directed, hence the edges have arrows. Like the previous plot here too docker, openshift and kubernetes repos seem to be connected and the other community repos are disconnected from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks = nx.pagerank(H, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first input parameter of the method, H, is the NetworkX graph. Undirected graphs will be converted to a directed graph with two directed edges for each undirected edge. The second parameter, alpha, is the damping parameter for PageRank and the default value is 0.85. The fourth parameter, max_iter, is the Maximum number of iterations. The seventh parameter, weight, represents the edge attribute that should be used as the edge weight. If it’s not specified, the weight of all edges will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topnodes = dict(sorted(pageranks.items(), key = itemgetter(1), reverse = True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to give higher priority to outgoing nodes. So we take 50 nodes with lowest pagerank (most number of connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = topnodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = nx.subgraph(H, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = subgraph.nodes()\n",
    "colors = []\n",
    "for n in nodes:\n",
    "    if n in repo_name_set_rh_affiliated:\n",
    "        colors.append('blue')\n",
    "    elif n in repo_name_set_interesting:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('yellow')\n",
    "\n",
    "node_sizes = [1000 if ns in repo_name_set else 50 for ns in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Top 20 repositories filtered by PageRank\", font)\n",
    "\n",
    "ax.legend(handles=[yellow_patch, blue_patch, green_patch])\n",
    "nx.draw_networkx(subgraph, node_color=colors, node_size=node_sizes, font_size=9, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "Here we see that some contributor nodes have been eliminated from the graph. However, all the repository nodes that we saw previously still remain. As we plot the top 20 nodes ranked by PageRank, it seems to have filtered out a lot of contributor nodes. However, the repo nodes still are top ranked.\n",
    "\n",
    "Lets look at what are the top 3 nodes\n",
    "\n",
    "The PageRank algorithm measures the importance of nodes by analyzing the quantity and quality of the links that point to it. So if a repository has a lot of contributors and especially if these contributors count as important nodes, they are still ranked high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topnodes = dict(sorted(pageranks.items(), key = itemgetter(1), reverse = True)[:3])\n",
    "topnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = topnodes.keys()\n",
    "subgraph = nx.subgraph(H, key)\n",
    "nodes = subgraph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Top 3 repositories based on PageRank\", font)\n",
    "\n",
    "nx.draw_networkx(subgraph, node_color=colors, font_size=9, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "As we plot the top 3 nodes, we see that 2 docker repos, and an unrelated community repo jetty-project show up. \n",
    "\n",
    "This is expected as PageRank assigns ranks to repositories by both analyzing the **number** of contributors connected to it as well as how **important those contributors** are within the network.\n",
    "\n",
    "The docker repos and jetty-project have a much larger number of contributors which seems to be influencing the ranks assigned by PageRank. It seems like PageRank is effective in showing us well-connected and prominent repositories.\n",
    "**However, PageRank may not be the most effective algorithm here to show us important emerging repositories (here openshift) in connection to existing well-known repos (here docker and kubernetes).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrality analysis provides information about the node’s importance for an information flow or connectivity of the network. Betweenness centrality measures the extent to which a node lies on paths between other nodes in the graph. Nodes with higher betweenness have more influence within a network. Thus repositories with higher centrality scores can thought to be influential in connection to other repositories in the network.\n",
    "\n",
    "This is a good metric for us, as using this we are able to better capture relative importance of repositories. In our case since we start with examples of well-known repos, we can use this algorithm to find other repos which are important in connection to these well-known repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_sizes = [v * 10000 for v in centrality.values()]\n",
    "\n",
    "nodes = G.nodes()\n",
    "\n",
    "colors = []\n",
    "for n in nodes:\n",
    "    if n in repo_name_set_rh_affiliated:\n",
    "        colors.append('blue')\n",
    "    elif n in repo_name_set_interesting:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('yellow')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "ax.legend(handles=[yellow_patch, blue_patch, green_patch])\n",
    "\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Nodes scaled on Betweenness Centrality scores\", font)\n",
    "nx.draw_networkx(G, node_color=colors, with_labels=False, node_size=node_sizes, font_size=9,  edge_color=\"gainsboro\", ax=ax, alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "The size of the nodes in the plot above indicate higher centrality scores. We see that the centrality scores highly rank the docker, kubernetes and openshift repos. \n",
    "\n",
    "Betweenness Centrality gives us good results and is highly ranking openshift repos in comparison to other community repos as this algorithm is able to better capture relative importance of repositories. \n",
    "\n",
    "By starting off with well-known repos, centrality is able to help us disvover other repos which are important in connection to these well-known repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Type 2: Nodes as projects edges as contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we represent data in another graph representation where the project repositories are represented by nodes and the edges are shared contributions between those projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributorGraph = {}\n",
    "for i, row in merged_df.iterrows():\n",
    "    if row['cntrb_id'] not in contributorGraph:\n",
    "        contributorGraph[row['cntrb_id']] = []\n",
    "    if(row['total_contributions'] > 0):\n",
    "        contributorGraph[row['cntrb_id']].append((row['repo_name'], row['total_contributions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(contributorGraph.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contributorGraph` above is a dictionary where each key is a contributor, and the value is a list of repositories the contributor has contributed to and the number of contributions it has made.\n",
    "\n",
    "\n",
    "Hence, if there are more than one repositories in the list that corresponds to a single contributor key, they can be thought of as **\"connected\"** project repositories and we will now calculate the number of **\"shared connections\"** between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structure of `contributorGraph` =  \n",
    "{  \n",
    "`contributor1`: [(`repo1`, `contributions by the contributor1 in repo 1`)],  \n",
    " `contributor2`: [(`repo2`, `contributions by the contributor2 in repo 2` ), (`repo1`, `contributions by the contributor2 in repo 1`)]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"shared connections\"** constitute of *commits*, *pull requests*, *issues* and *pull request reviews* that are made by the same contributor.\n",
    "We will call 2 project repositories **\"connected\"** if they have a **\"shared connection\"** between them. \n",
    "This means if they have a contributor who makes a *commit*, *pull request*, *issue* or *pull request review* in both the repositories, they count as a shared contributor and the repositories are connected. \n",
    "\n",
    "We track the number of shared contributions between 2 repositories for creating this graph plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonRepoContributionsByContributor = collections.defaultdict(int)\n",
    "for key in contributorGraph:\n",
    "    if len(contributorGraph[key])-1 <= 0:\n",
    "        continue\n",
    "    for repoContributionIndex in range(len(contributorGraph[key])-1):\n",
    "        commonRepoContributionsByContributor[(contributorGraph[key][repoContributionIndex][0], contributorGraph[key][repoContributionIndex+1][0])] += contributorGraph[key][repoContributionIndex][1]+contributorGraph[key][repoContributionIndex+1][1]\n",
    "print(commonRepoContributionsByContributor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`commonRepoContributionsByContributor` is a nested dictionary consisting of dictionaries of repository pairs and their common contributions. \n",
    "\n",
    "structure of `commonRepoContributionsByContributor` =  \n",
    "{  \n",
    "(`repo1, repo2`): `PRs by same authors in repo 1 and repo 2`,  \n",
    "(`repo2, repo4`): `PRs by same authors in repo 2 and repo 4`,  \n",
    "(`repo2, repo5`): `PRs by same authors in repo 2 and repo 5`,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for key in commonRepoContributionsByContributor:\n",
    "    res.append(tuple(str(k) for k in list(key)) + (commonRepoContributionsByContributor[key],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting the graph below, we pick the repositories as the nodes and let the shared contributions dictate the edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_weighted_edges_from(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = g.nodes()\n",
    "colors = []\n",
    "\n",
    "for n in nodes:\n",
    "    if n in repo_name_set_rh_affiliated:\n",
    "        colors.append('blue')\n",
    "    elif n in repo_name_set_interesting:\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "ax.set_title(\"Graph with Nodes as projects and edges as number of contributions\", font)\n",
    "\n",
    "ax.legend(handles=[blue_patch, green_patch])\n",
    "nx.draw_networkx(g, node_size=180, node_color=colors, font_size=12, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What do we see in this plot?***\n",
    "\n",
    "The above graph represents project repositories and how close or far they are to each other based on their degree of connection (number of shared contributions amongst them). If 2 nodes are close to each other, the 2 projects have a high number of shared contributions and vice versa. Each node in this graph has atleast one connection. We only plot project repositories which are connected to existing known repositories.  \n",
    "\n",
    "We see that this graph representation effectively filters out the repositories we are most interested in seeing. **The repository \"closest\" to kubernetes and docker repositories are 2 OpenShift repositories \"installer\" followed by \"source-to-image\" and \"osin\" and the other unrelated community repositories do not appear on the plot as they are not \"connected\" to kubernetes**\n",
    "\n",
    "Even if there were common contributors between the other repos and our well-known repos, the edge lengths and the distance between the nodes can be used to filter out the most connected repos that we are interested in.\n",
    "\n",
    "Thus, this graph representation turns out to be an effective way to filter out emerging repositories in relation to already prominent communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we ran an experiment on known open source GitHub repositories. We looked at a timeframe in history where a new open source community was emerging/becoming popular and we tried to track the rise of the community using graph alrgorithms.\n",
    "\n",
    "As an example, we wanted to track the emergence of OpenShift as the downstream of Kubernetes. So we started with some Kubernetes repos, Docker repos, OpenShift repos and some other unrelated repos with the hope that as a result of the analysis, OpenShift will emerge as the community closest in connection to Kubernetes and Docker, and the other repos although popular or important in their own context do not emerge.\n",
    "\n",
    "As a result of creating various types of graphical representations of the Github data and using algorithms like PageRank and Betweenness centrality, we saw that:\n",
    "\n",
    "1. **PageRank is not effective in showing us important emerging OpenShift repositories in relation to Kubernetes and Docker.**\n",
    "\n",
    "2. **Using Betweenness Centrality ranks, we are able to get OpenShift, Kubernetes and Docker repos in the top most highly ranked or central repos.**\n",
    "\n",
    "3. **Using the graph representation type 2 where we represent nodes as projects edges as contributors, we are able to very effectively filter out the repositories we are most interested in seeing ie Kubernetes, Docker and OpenShift repos.**\n",
    "\n",
    "    **We see that the repositories \"closest\" to Kubernetes and Docker repos are 3 OpenShift repositories \"installer\", followed by \"source-to-image\" followed by \"osin\" and the other unrelated community repositories do not appear on the plot.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
