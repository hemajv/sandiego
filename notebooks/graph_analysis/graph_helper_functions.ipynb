{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6875b1a-f1fe-4779-a4bb-17c5a9c8ee70",
   "metadata": {},
   "source": [
    "# Graph Analysis Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17ef2b5-015c-4b1c-886c-ac8553b869e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../comm_cage.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-858f614c4a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../comm_cage.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../comm_cage.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sqlalchemy as salc\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "with open(\"../../comm_cage.json\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908df105-d4d6-48c6-b322-7272cb026dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos(repos, engine):\n",
    "\n",
    "    repo_set = []\n",
    "    repo_name_set = []\n",
    "    for repo_git in repos:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                     SET SCHEMA 'augur_data';\n",
    "                     SELECT \n",
    "                        b.repo_id,\n",
    "                        b.repo_name\n",
    "                    FROM\n",
    "                        repo_groups a,\n",
    "                        repo b\n",
    "                    WHERE\n",
    "                        a.repo_group_id = b.repo_group_id AND\n",
    "                        b.repo_git = \\'{repo_git}\\'\n",
    "            \"\"\")\n",
    "\n",
    "        t = engine.execute(repo_query)\n",
    "        results = t.mappings().all()[0]\n",
    "        repo_id = results['repo_id']\n",
    "        repo_name = results['repo_name']\n",
    "        repo_set.append(repo_id)\n",
    "        repo_name_set.append(repo_name)\n",
    "    return repo_set, repo_name_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a5c30d-9e9d-46b2-916a-ff64809bc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_contributors(repo_set, engine):\n",
    "\n",
    "    issue_contrib = pd.DataFrame()\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    ie.cntrb_id,\n",
    "                    ie.action,\n",
    "                    i.issue_id,\n",
    "                    i.created_at\n",
    "                    FROM\n",
    "                    repo r, issues i, issue_events ie\n",
    "                     WHERE\n",
    "                    i.repo_id = \\'{repo_id}\\' AND\n",
    "                    i.repo_id = r.repo_id AND\n",
    "                    i.issue_id = ie.issue_id AND\n",
    "                    ie.action='closed'\n",
    "            \"\"\")\n",
    "        df_current_repo = pd.read_sql(repo_query, con=engine)\n",
    "        issue_contrib = pd.concat([issue_contrib, df_current_repo])\n",
    "\n",
    "    issue_contrib = issue_contrib.reset_index()\n",
    "    issue_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    issue_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'action', 'issue_id', 'created_at']\n",
    "#     issue_contrib['cntrb_id'] = issue_contrib['cntrb_id'].astype('Int64')\n",
    "    return issue_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e43d1c-e03d-4428-a600-dfd40f9ae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos_outside(engine):\n",
    "\n",
    "    issue_contrib = pd.DataFrame()\n",
    "    repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_name,\n",
    "                    (CASE WHEN REGEXP_LIKE(repo_name, 'https://github.com/open-telemetry/opentelemetry-go|https://github.com/open-telemetry/opentelemetry-specification|https://github.com/open-telemetry/opentelemetry-collector') THEN true ELSE NULL\n",
    "                    END) AS flag\n",
    "                    FROM repo r\n",
    "            \"\"\")\n",
    "    df_current_repo = pd.read_sql(repo_query, con=engine)\n",
    "        \n",
    "    print(df_current_repo)\n",
    "#     issue_contrib['cntrb_id'] = issue_contrib['cntrb_id'].astype('Int64')\n",
    "    return issue_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b397d6d4-320a-4434-a1ff-5bd006eddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_contributors(repo_set, engine):\n",
    "\n",
    "    pr_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    prm.cntrb_id,\n",
    "                    prm.pull_request_id,\n",
    "                    pr.pr_created_at\n",
    "                    FROM\n",
    "                    repo r, pull_request_meta prm, pull_requests pr\n",
    "                    WHERE\n",
    "                    prm.repo_id = \\'{repo_id}\\' AND\n",
    "                    prm.repo_id = r.repo_id AND\n",
    "                    prm.pull_request_id = pr.pull_request_id\n",
    "            \"\"\")\n",
    "        df_current_repo = pd.read_sql(repo_query, con=engine)\n",
    "        pr_contrib = pd.concat([pr_contrib, df_current_repo])\n",
    "\n",
    "    pr_contrib = pr_contrib.reset_index()\n",
    "    pr_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    pr_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'pull_request_id', 'pr_created_at']\n",
    "\n",
    "    return pr_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87158ad6-c882-4f36-97d3-26f6eaa96dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_contributors(repo_set, engine):\n",
    "\n",
    "    commit_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    ca.cntrb_id,\n",
    "                    c.cmt_id,\n",
    "                    c.cmt_date_attempted\n",
    "                    FROM\n",
    "                    repo r, commits c, contributors_aliases ca\n",
    "                    WHERE\n",
    "                    c.repo_id = \\'{repo_id}\\' AND\n",
    "                    c.repo_id = r.repo_id and\n",
    "                    c.cmt_committer_email = ca.alias_email\n",
    "            \"\"\")\n",
    "        df_current_repo = pd.read_sql(repo_query, con=engine)\n",
    "        commit_contrib = pd.concat([commit_contrib, df_current_repo])\n",
    "\n",
    "    commit_contrib = commit_contrib.reset_index()\n",
    "    commit_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    commit_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'cmt_id', 'cmt_date_attempted']\n",
    "\n",
    "    return commit_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a73e2b4-a6de-4048-aab7-bec1bb2aa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prr_contributors(repo_set, engine):\n",
    "\n",
    "    prr_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    prr.cntrb_id,\n",
    "                    prr.pull_request_id\n",
    "                    FROM\n",
    "                    repo r, pull_request_reviewers prr\n",
    "                    WHERE\n",
    "                    prr.repo_id = \\'{repo_id}\\' AND\n",
    "                    prr.repo_id = r.repo_id\n",
    "            \"\"\")\n",
    "        df_current_repo = pd.read_sql(repo_query, con=engine)\n",
    "        prr_contrib = pd.concat([prr_contrib, df_current_repo])\n",
    "\n",
    "    prr_contrib = prr_contrib.reset_index()\n",
    "    prr_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    prr_contrib.columns = ['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'pull_request_id']\n",
    "\n",
    "    return prr_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb85e401-d717-47a5-8639-cc23555de47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_melted_dfs(df):\n",
    "\n",
    "    df = df.groupby(['repo_name', 'cntrb_id']).size().unstack(fill_value=0)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df_melted = df.melt(['repo_name'], var_name = 'cntrb_id',value_name='number')\n",
    "    df_melted = df_melted[df_melted[df_melted.columns[2]] != 0]\n",
    "\n",
    "    return df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f88f1f5-9cf6-46ba-8d9b-bed7b45a3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_ranks(graph, top, graduated_repos, incubating_repos, sandbox_repos, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by page rank \n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the page rank\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    pageranks = nx.pagerank(graph, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None)\n",
    "    \n",
    "    scores['page_rank'] = scores['repo'].map(pageranks)\n",
    "    \n",
    "    page_rank_graduated_repos = collections.defaultdict(int)\n",
    "    page_rank_incubating_repos = collections.defaultdict(int)\n",
    "    page_rank_sandbox_repos = collections.defaultdict(int)\n",
    "\n",
    "    for key in pageranks:\n",
    "        if key in graduated_repos:\n",
    "            page_rank_graduated_repos[key] = pageranks[key]\n",
    "\n",
    "        elif key in incubating_repos:\n",
    "            page_rank_incubating_repos[key] = pageranks[key]\n",
    "        elif key in sandbox_repos:\n",
    "            page_rank_sandbox_repos[key] = pageranks[key]\n",
    "    \n",
    "    top_page_rank_graduated_repos = dict(sorted(page_rank_graduated_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_page_rank_incubating_repos = dict(sorted(page_rank_incubating_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_page_rank_sandbox_repos = dict(sorted(page_rank_sandbox_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    \n",
    "    return top_page_rank_graduated_repos, top_page_rank_incubating_repos, top_page_rank_sandbox_repos, pageranks, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e532825c-411d-4cce-b659-31ea239ba910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweenness_centrality(graph, top, graduated_repos, incubating_repos, sandbox_repos, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by betweenness centrality scores\n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the betweenness centrality scores\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    # Betweenness centrality measures the extent to which a node lies on paths between other nodes in the graph. \n",
    "    # Nodes with higher betweenness have more influence within a network. \n",
    "    # Thus repositories with higher centrality scores can thought to be influential in connection to other repositories in the network.\n",
    "\n",
    "    bw_centrality = nx.betweenness_centrality(graph)\n",
    "    \n",
    "    scores['betweenness_centrality'] = scores['repo'].map(bw_centrality)\n",
    "\n",
    "    bw_centrality_graduated_repos = collections.defaultdict(int)\n",
    "    bw_centrality_incubating_repos = collections.defaultdict(int)\n",
    "    bw_centrality_sandbox_repos = collections.defaultdict(int)\n",
    "\n",
    "    for key in bw_centrality:\n",
    "        if key in graduated_repos:\n",
    "            bw_centrality_graduated_repos[key] = bw_centrality[key]\n",
    "\n",
    "        elif key in incubating_repos:\n",
    "            bw_centrality_incubating_repos[key] = bw_centrality[key]\n",
    "        \n",
    "        elif key in sandbox_repos:\n",
    "            bw_centrality_sandbox_repos[key] = bw_centrality[key]\n",
    "    \n",
    "    top_bw_centrality_graduated_repos = dict(sorted(bw_centrality_graduated_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_bw_centrality_incubating_repos = dict(sorted(bw_centrality_incubating_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_bw_centrality_sandbox_repos = dict(sorted(bw_centrality_sandbox_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    \n",
    "    return top_bw_centrality_graduated_repos, top_bw_centrality_incubating_repos, top_bw_centrality_sandbox_repos, bw_centrality, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e83c78-0404-40cb-8e5e-71a9f11d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closeness_centrality(graph, top, graduated_repos, incubating_repos, sandbox_repos, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by closeness centrality scores\n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the closeness centrality scores\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    c_centrality = nx.closeness_centrality(graph)\n",
    "    \n",
    "    scores['closeness_centrality'] = scores['repo'].map(c_centrality)\n",
    "    \n",
    "    c_centrality_graduated_repos = collections.defaultdict(int)\n",
    "    c_centrality_incubating_repos = collections.defaultdict(int)\n",
    "    c_centrality_sandbox_repos = collections.defaultdict(int)\n",
    "\n",
    "    for key in c_centrality:\n",
    "        if key in graduated_repos:\n",
    "            c_centrality_graduated_repos[key] = c_centrality[key]\n",
    "\n",
    "        elif key in incubating_repos:\n",
    "            c_centrality_incubating_repos[key] = c_centrality[key]\n",
    "        \n",
    "        elif key in sandbox_repos:\n",
    "            c_centrality_sandbox_repos[key] = c_centrality[key]\n",
    "    \n",
    "    top_c_centrality_graduated_repos = dict(sorted(c_centrality_graduated_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_c_centrality_incubating_repos = dict(sorted(c_centrality_incubating_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    top_c_centrality_sandbox_repos = dict(sorted(c_centrality_sandbox_repos.items(), key = itemgetter(1), reverse = True)[:top])\n",
    "    \n",
    "    return top_c_centrality_graduated_repos, top_c_centrality_incubating_repos, top_c_centrality_sandbox_repos, c_centrality, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0379ee41-df81-497c-b87f-587f872ada8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph, graduated_repos, incubating_repos, sandbox_repos, size, title, weights=None, with_labels=True, alpha=None, edge_color='k'):\n",
    "    \n",
    "    \"\"\"\n",
    "    graph: the networkX graph that we want to plot\n",
    "    known_repos: list of known repos for coloring\n",
    "    other_repos: list of other repos for coloring\n",
    "    size: can be either 'weighted', 'equal' or 'conditional'\n",
    "    When size is 'weighted', the node sizes on the graph are based on the weights provided\n",
    "    When size is 'equal', all nodes are the same size\n",
    "    When size is 'conditional', nodes which belong to the weights array are larger than the rest of the nodes\n",
    "    weights: this decides the size of the nodes in the 'weighted' and 'conditional' type sizes\n",
    "    \n",
    "    here we plot a networkx graph based on the provided parameters\n",
    "    \"\"\"\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Graduated Repositories')\n",
    "    green_patch = mpatches.Patch(color='green', label='Incubating Repositories')\n",
    "    red_patch = mpatches.Patch(color='red', label='Sandbox Repositories')\n",
    "    yellow_patch = mpatches.Patch(color='green', label='Contributors')\n",
    "\n",
    "    nodes = graph.nodes()\n",
    "    colors = []\n",
    "    for n in nodes:\n",
    "        if n in graduated_repos:\n",
    "            colors.append('blue')\n",
    "        elif n in incubating_repos:\n",
    "            colors.append('green')\n",
    "        elif n in sandbox_repos:\n",
    "            colors.append('red')\n",
    "        else:\n",
    "            colors.append('yellow')\n",
    "            \n",
    "    if size == 'weighted':\n",
    "        node_sizes = [v * 10000 for v in weights.values()]\n",
    "    elif size == 'conditional':\n",
    "        node_sizes = [1000 if ns in weights else 50 for ns in nodes]\n",
    "    elif size == 'equal':\n",
    "        node_sizes = 300\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "    font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "    \n",
    "    ax.set_title(title, font)\n",
    "    ax.legend(handles=[yellow_patch, blue_patch, green_patch, red_patch])\n",
    "    \n",
    "    nx.draw_networkx(graph, node_color=colors, node_size=node_sizes, font_size=9, ax=ax, with_labels=with_labels, alpha=alpha, edge_color=edge_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a562a663-b92e-4323-92a2-7e7c58ed1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_nodes_edges_contributions(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using this function we represent data as a graph where the project repositories are represented by nodes \n",
    "    and the edges are shared contributions between those projects\n",
    "    \"\"\"\n",
    " \n",
    "    # structure of `contributorGraph` =  \n",
    "    # {  \n",
    "    # `contributor1`: [(`repo1`, `contributions by the contributor1 in repo 1`)],  \n",
    "    #  `contributor2`: [(`repo2`, `contributions by the contributor2 in repo 2` ), (`repo1`, `contributions by the contributor2 in repo 1`)]  \n",
    "    # }\n",
    "\n",
    "    contributorGraph = {}\n",
    "    for i, row in df.iterrows():\n",
    "        if row['cntrb_id'] not in contributorGraph:\n",
    "            contributorGraph[row['cntrb_id']] = []\n",
    "        if(row['total_contributions'] > 0):\n",
    "            contributorGraph[row['cntrb_id']].append((row['repo_name'], row['total_contributions']))\n",
    "            \n",
    "    # `contributorGraph`  is a dictionary where each key is a contributor, \n",
    "    #  and the value is a list of repositories the contributor has contributed to and the number of contributions it has made.\n",
    "    \n",
    "    #  \"shared connections\" constitute of commits, PRs, issues* and PR reviews that are made by the same contributor.\n",
    "    #  2 project repositories are \"connected\" if they have a \"shared connection\"** between them. \n",
    "    #  If they have a contributor who makes a commit, PR, issue or PR review in both the repositories, \n",
    "    #  they count as a shared contributor and the repositories are connected. \n",
    "    \n",
    "    commonRepoContributionsByContributor = collections.defaultdict(int)\n",
    "    for key in contributorGraph:\n",
    "        if len(contributorGraph[key])-1 <= 0:\n",
    "            continue\n",
    "        for repoContributionIndex in range(len(contributorGraph[key])-1):\n",
    "            commonRepoContributionsByContributor[(contributorGraph[key][repoContributionIndex][0], contributorGraph[key][repoContributionIndex+1][0])] += contributorGraph[key][repoContributionIndex][1]+contributorGraph[key][repoContributionIndex+1][1]\n",
    "\n",
    "    # `commonRepoContributionsByContributor` is a nested dictionary consisting of dictionaries of repository pairs and their common contributions. \n",
    "    #  structure of `commonRepoContributionsByContributor` =  \n",
    "    #  {  \n",
    "    #  (`repo1, repo2`): `PRs by same authors in repo 1 and repo 2`,  \n",
    "    #  (`repo2, repo4`): `PRs by same authors in repo 2 and repo 4`,  \n",
    "    #  (`repo2, repo5`): `PRs by same authors in repo 2 and repo 5`,   \n",
    "    #   }    \n",
    "    \n",
    "    res = []\n",
    "    for key in commonRepoContributionsByContributor:\n",
    "        res.append(tuple(str(k) for k in list(key)) + (commonRepoContributionsByContributor[key],))\n",
    "        \n",
    "    return res, commonRepoContributionsByContributor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
